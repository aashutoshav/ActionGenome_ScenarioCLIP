# Create Chunks for OpenPSG for Data Processing 
The two jsons ( **train_details.json, train_chunks.json** ) generated by  running the folowing script are useful for the rest of the pipeline. Skip this step if the chunks are already generated.

```
python3 ./dataset/train_json.py \
    --root_folder "/netscratch/ali/coco" \
    --out_details_json_path  train_details.json \
    --out_chunks_json_path train_chunks.json 
```

# Run the Data/Annotations Generation Pipeline

## Step 1
*   Run the Gemma Model in the `src folder` for Generating (a) List of Objects (b) Relations, and (c) Action Caption. 

*    Go into the src folder with the following command
    ```cd src```

    ```
    python3 ./gemma.py 
    --root_dir "/netcratch/ali/coco"
    --storage_dir "/netcratch/ali/ScenarioCLIP_ResultsAnalysis" \
    --chunk_json_path "/netscratch/ali/coco/train_chunks.json" \
    --metadata_json_path "/netscratch/ali/coco/train_details.json" \
    --batch_size 1 \
    --cache_dir "" \
    --max_workers 8 \
    --start_chunk 2 \
    --end_chunk 3 \
    --gpu 0
    ```

    This command will run the VLM on the chunks from start_chunk to end_chunk, both inclusive

## Step 2
*   Run the ColorMapping Code for generating an object-color-map for each label and stores it in a certain model. 
* This code also lists out all the objects, actions and relations and stores it in a json
* Important: This code file is present outside the src folder

    ```
    python3 ./mapping.py 
    --path_to_llama_jsons "/netcratch/ali/ScenarioCLIP_ResultsAnalysis" \
    --labels_path "/netscratch/ali/coco/object_action_list.json" \
    --colorMapping_path "/netscratch/ali/coco/colorMapping.json" \
    ```

## Step 3
*   Run the GroundingDino Model in the `src folder` for generating bounding boxes for these labels.
*   The img_save argument should be True if you want to save the bounding-box visualisation in a jpg file for each image

    ```
    python3 ./grounding_dino.py 
    --root_dir "/netcratch/ali/coco"
    --storage_dir "/netcratch/ali/ScenarioCLIP_ResultsAnalysis" \
    --chunk_json_path "/netscratch/ali/coco/train_chunks.json" \
    --metadata_json_path "/netscratch/ali/coco/train_details.json" \
    --color_mapping_json "/netscratch/ali/coco/colorMapping.json" \
    --batch_size 8 \
    --start_chunk 0 \
    --end_chunk 29 \
    --cache_dir "" \
    --img_save False \
    --gpu 0
    ```

## Step 4
*   Run the GroundedSAM Model in the `src folder` for generating segmentation masks for these labels.

    ```
    python3 ./grounding_sam.py 
    --root_dir "/netcratch/ali/coco"
    --storage_dir "/netcratch/ali/ScenarioCLIP_ResultsAnalysis" \
    --chunk_json_path "/netscratch/ali/coco/train_chunks.json" \
    --metadata_json_path "/netscratch/ali/coco/train_details.json" \
    --color_mapping_json "/netscratch/ali/coco/colorMapping.json" \
    --batch_size 6 \
    --max_workers 8 \
    --cache_dir "" \
    --start_chunk 0 \
    --end_chunk 29 \
    --gpu 0
    ```

## Step 5 (Optional)

*   To check masks of individual objects in an image using SAM outputs, we can experiment with, and change the `mask_visualisation.py` in the dataset folder, by changing the sam_path and img_path in the main function


## Step 6 (Optional)

*   To change the method of forming the focused regions, run the focused_regions.py file in the src folder, which implements the masks on the labels corresponding to individual relations opaqualy instead of using the radial basis function like implemented in the SAM Code

<br>
<br>

# Visualisation Code

* Run the `visualisation.py` file with function name arguments for visualisation of different stages of the pipeline, by passing the desired paths for each of the functions

### Option 1 (Visualise Bounding Box):

```
python visualisation.py visualise_bbox \
--img_path "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/vanilla.jpg" \
--dino_json_path "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/dino_results/vanilla_grounding_dino.json" \
--color_mapping_json "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/color_mapping.json" \
--output_dir "./output" \
```

### Option 2 (Visualise all masks):

```
python visualisation.py show_masks_on_image \
--image_path "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/vanilla.jpg" \
--npz_file "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/sam_results/vanilla_grounding_sam.npz" \
--colorMappingPath "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/color_mapping.json" \
--output_path "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/vanilla_masked.jpg" \
```

### Option 3 (Generate all Focused Region Images):

```
python visualisation.py generate_relations \
--img_path "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/vanilla.jpg" \
--npz_file "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/sam_results/vanilla_grounding_sam.npz" \
--dino_path "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/dino_results/vanilla_grounding_dino.json" \
--llama_path "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/gemma_jsons/vanilla_gemma.json" \
--output_folder "./output" \
```

### Option 4 (Generate Focused Regions as subplots)

```
python visualisation.py generate_relations_with_subplots \
--img_path "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/vanilla.jpg" \
--npz_file "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/sam_results/vanilla_grounding_sam.npz" \
--dino_path "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/dino_results/vanilla_grounding_dino.json" \
--llama_path "/home/zeta/Workbenches/ActionGenome_ScenarioCLIP/ActionGenome_DataSet/src/vanilla/gemma_jsons/vanilla_gemma.json" \
--output_folder "./output" \
```

<br>
<br>

# Dataset Storage Structure (OpenPSG)

## Ground Truth Images

```
/root_dir/ 
│ 
├── train2017/ 
│   ├── 000000000001.jpg 
│   ├── 000000000002.jpg 
│   └── ...                        
           
```

## Action Genome Captions
```
/storage_dir/ 
│ 
├── gemma_jsons/ 
│   ├── 000000000001_gemma.json 
│   ├── 000000000002_gemma.json 
│   └── ... 
├── dino_results/ 
│   ├── 000000000001_grounding_dino.json 
│   ├── 000000000002_grounding_dino.json 
│   └── ... 
├── sam_results/ 
│   ├── 000000000001_grounding_sam.npz 
│   ├── 000000000001/ 
|   |   ├── 000000000001_relations_0.png 
|   |   ├── 000000000001_relations_1.png 
|   |   ├── ... 
│   ├── 000000000002_grounding_sam.npz 
│   ├── 000000000002/ 
|   |   ├── 000000000002_relations_0.png 
|   |   ├── 000000000002_relations_1.png 
|   |   ├── ... 
│   └── ... 
├── bbox_visualised/ 
│   ├── 000000000001_bbox.jpg 
│   ├── 000000000002_bbox.jpg 
│   └── ... 
```

# Running Generation for a random non-dataset image

## Data Storage Structure

```
/vanilla/ 
│ 
├── vanilla.jpg 
```

<!-- # Dataset Storage Structure (MMT) -->
